{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#datarepo-a-simple-platform-for-complex-data","title":"datarepo: a simple platform for complex data","text":"<p><code>datarepo</code> is a simple query interface for multimodal data at any scale.</p> <p>With <code>datarepo</code>, you can define a catalog, databases, and tables to query any existing data source. Once you've defined your catalog, you can spin up a static site for easy browsing or a read-only API for programmatic access. No running servers or services!</p> <p>The <code>datarepo</code> catalog has native, declarative connectors to Delta Lake and Parquet stores. <code>datarepo</code> also supports defining tables via custom Python functions, so you can connect to any data source!</p> <p>Here's an example catalog:</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Unified interface: Query data across different storage modalities (Parquet, DeltaLake, relational databases)</li> <li>Declarative catalog syntax: Define catalogs in python without running services</li> <li>Catalog site generation: Generate a static site catalog for visual browsing</li> <li>Extensible: Declare tables as custom python functions for querying any data</li> <li>API support: Generate a YAML config for querying with ROAPI</li> <li>Fast: Uses Rust-native libraries such as polars, delta-rs, and Apache DataFusion for performant reads</li> </ul>"},{"location":"#philosophy","title":"Philosophy","text":"<p>Data engineering should be simple. That means:</p> <ol> <li>Scale up and scale down - tools should scale down to a developer's laptop and up to stateless clusters</li> <li>Prioritize local development experience - use composable libraries instead of distributed services</li> <li>Code as a catalog - define tables in code, generate a static site catalog and APIs without running services</li> </ol>"},{"location":"#quick-start","title":"Quick start","text":"<p>Install the latest version with:</p> <pre><code>pip install data-repository\n</code></pre>"},{"location":"#create-a-table-and-catalog","title":"Create a table and catalog","text":"<p>First, create a module to define your tables (e.g., <code>tpch_tables.py</code>):</p> <pre><code># tpch_tables.py\nfrom datarepo.core import (\n    DeltalakeTable,\n    ParquetTable,\n    Filter,\n    table,\n    NlkDataFrame,\n    Partition,\n    PartitioningScheme,\n)\nimport pyarrow as pa\nimport polars as pl\n\n# Delta Lake backed table\npart = DeltalakeTable(\n    name=\"part\",\n    uri=\"s3://my-bucket/tpc-h/part\",\n    schema=pa.schema(\n        [\n            (\"p_partkey\", pa.int64()),\n            (\"p_name\", pa.string()),\n            (\"p_mfgr\", pa.string()),\n            (\"p_brand\", pa.string()),\n            (\"p_type\", pa.string()),\n            (\"p_size\", pa.int32()),\n            (\"p_container\", pa.string()),\n            (\"p_retailprice\", pa.decimal128(12, 2)),\n            (\"p_comment\", pa.string()),\n        ]\n    ),\n    docs_filters=[\n        Filter(\"p_partkey\", \"=\", 1),\n        Filter(\"p_brand\", \"=\", \"Brand#1\"),\n    ],\n    unique_columns=[\"p_partkey\"],\n    description=\"\"\"\n    Part information from the TPC-H benchmark.\n    Contains details about parts including name, manufacturer, brand, and retail price.\n    \"\"\",\n    table_metadata_args={\n        \"data_input\": \"Part catalog data from manufacturing systems, updated daily\",\n        \"latency_info\": \"Daily batch updates from manufacturing ERP system\",\n        \"example_notebook\": \"https://example.com/notebooks/part_analysis.ipynb\",\n    },\n)\n\n# Table defined as a function\n@table(\n    data_input=\"Supplier master data from vendor management system &lt;code&gt;/api/suppliers/master&lt;/code&gt; endpoint\",\n    latency_info=\"Updated weekly by the supplier_master_sync DAG on Airflow\",\n)\ndef supplier() -&gt; NlkDataFrame:\n    \"\"\"Supplier information from the TPC-H benchmark.\"\"\"\n    data = {\n        \"s_suppkey\": [1, 2, 3, 4, 5],\n        \"s_name\": [\n            \"Supplier#1\",\n            \"Supplier#2\",\n        ],\n        \"s_address\": [\n            \"123 Main St\",\n            \"456 Oak Ave\",\n        ],\n        \"s_nationkey\": [1, 1],\n        \"s_phone\": [\"555-0001\", \"555-0002\"],\n        \"s_acctbal\": [1000.00, 2000.00],\n        \"s_comment\": [\"Comment 1\", \"Comment 2\"],\n    }\n    return pl.LazyFrame(data)\n</code></pre> <pre><code># tpch_catalog.py\nfrom datarepo.core import Catalog, ModuleDatabase\nimport tpch_tables\n\n# Create a catalog\ndbs = {\"tpc-h\": ModuleDatabase(tpch_tables)}\nTPCHCatalog = Catalog(dbs)\n</code></pre>"},{"location":"#query-the-data","title":"Query the data","text":"<pre><code>&gt;&gt;&gt; from tpch_catalog import TPCHCatalog\n&gt;&gt;&gt; from datarepo.core import Filter\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get part and supplier information\n&gt;&gt;&gt; part_data = TCPHCatalog.db(\"tpc-h\").table(\n...     \"part\",\n...     (\n...         Filter('p_partkey', 'in', [1, 2, 3, 4]),\n...         Filter('p_brand', 'in', ['Brand#1', 'Brand#2', 'Brand#3']),\n...     ),\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; supplier_data = TCPHCatalog.db(\"tpc-h\").table(\"supplier\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Join part and supplier data and select specific columns\n&gt;&gt;&gt; joined_data = part_data.join(\n...     supplier_data,\n...     left_on=\"p_partkey\",\n...     right_on=\"s_suppkey\",\n... ).select([\"p_name\", \"p_brand\", \"s_name\"]).collect()\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(joined_data)\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 p_name     \u2502 p_brand    \u2502 s_name     \u2502\n\u2502 ---        \u2502 ---        \u2502 ---        \u2502\n\u2502 str        \u2502 str        \u2502 str        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Part#1     \u2502 Brand#1    \u2502 Supplier#1 \u2502\n\u2502 Part#2     \u2502 Brand#2    \u2502 Supplier#2 \u2502\n\u2502 Part#3     \u2502 Brand#3    \u2502 Supplier#3 \u2502\n\u2502 Part#4     \u2502 Brand#1    \u2502 Supplier#4 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#generate-a-static-site-catalog","title":"Generate a static site catalog","text":"<p>You can export your catalog to a static site with a single command:</p> <pre><code># export.py\nfrom datarepo.export.web import export_and_generate_site\nfrom tpch_catalog import TPCHCatalog\n\n# Export and generate the site\nexport_and_generate_site(\n    catalogs=[(\"tpch\", TPCHCatalog)], output_dir=str(output_dir)\n)\n</code></pre>"},{"location":"#generate-an-api","title":"Generate an API","text":"<p>You can also generate a YAML configuration for ROAPI:</p> <pre><code>from datarepo.export import roapi\nfrom tpch_catalog import TPCHCatalog\n\n# Generate ROAPI config\nroapi.generate_config(TPCHCatalog, output_file=\"roapi-config.yaml\")\n</code></pre>"},{"location":"#about-neuralink","title":"About Neuralink","text":"<p><code>datarepo</code> is part of Neuralink's commitment to the open source community. By maintaining free and open source software, we aim to accelerate data engineering and biotechnology.</p> <p>Neuralink is creating a generalized brain interface to restore autonomy to those with unmet medical needs today, and to unlock human potential tomorrow.</p> <p>You don't have to be a brain surgeon to work at Neuralink. We are looking for exceptional individuals from many fields, including software and data engineering. Learn more at neuralink.com/careers.</p>"},{"location":"api-docs/","title":"API docs: reference documentation for classes and functions of datarepo project.","text":""},{"location":"api-docs/#databases-catalog-module","title":"Databases catalog module","text":""},{"location":"api-docs/#datarepo.core.catalog.Catalog","title":"<code>Catalog</code>","text":"<p>A catalog that manages multiple databases and provides access to their tables.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>class Catalog:\n    \"\"\"A catalog that manages multiple databases and provides access to their tables.\"\"\"\n\n    def __init__(\n        self, dbs: dict[str, Database], metadata: Optional[CatalogMetadata] = None\n    ):\n        \"\"\"Initialize the Catalog.\n\n        Example usage:\n            ``` py\n            from datarepo.core import Catalog, CatalogMetadata\n            catalog = Catalog(\n                dbs={\n                    \"my_db\": ModuleDatabase(my_database_module),\n                },\n                metadata=CatalogMetadata(jupyterhub_url=\"https://jupyterhub.example.com\")\n            )\n            ```\n\n        Args:\n            dbs (dict[str, Database]): A dictionary of database names and their corresponding Database objects.\n            metadata (Optional[CatalogMetadata], optional): Metadata for the catalog. Defaults to None.\n        \"\"\"\n        self._dbs = dbs\n        self._metadata = metadata or CatalogMetadata()\n        self._global_args: dict[str, Any] | None = None\n\n    def set_global_args(self, global_args: dict[str, Any]) -&gt; None:\n        \"\"\"Set global arguments for all database queries.\n\n        Args:\n            global_args (dict[str, Any]): A dictionary of global arguments to apply to all database queries.\n        \"\"\"\n        self._global_args = global_args\n\n    def db(self, db_name: str) -&gt; Database:\n        \"\"\"Get a database from the catalog.\n\n        Args:\n            db_name (str): The name of the database.\n\n        Raises:\n            KeyError: If the database is not found.\n\n        Returns:\n            Database: The requested database.\n        \"\"\"\n        db = self._dbs.get(db_name)\n\n        if db is None:\n            raise KeyError(\n                f\"Database '{db_name}' not found. Available databases: {self.dbs()}\"\n            )\n\n        if self._global_args is None:\n            return db\n\n        return DatabaseWithGlobalArgs(db, self._global_args)\n\n    def dbs(self) -&gt; list[str]:\n        \"\"\"Get a list of database names in the catalog.\n\n        Returns:\n            list[str]: A list of database names.\n        \"\"\"\n        return list(self._dbs.keys())\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Catalog.__init__","title":"<code>__init__(dbs, metadata=None)</code>","text":"<p>Initialize the Catalog.</p> Example usage <pre><code>from datarepo.core import Catalog, CatalogMetadata\ncatalog = Catalog(\n    dbs={\n        \"my_db\": ModuleDatabase(my_database_module),\n    },\n    metadata=CatalogMetadata(jupyterhub_url=\"https://jupyterhub.example.com\")\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dbs</code> <code>dict[str, Database]</code> <p>A dictionary of database names and their corresponding Database objects.</p> required <code>metadata</code> <code>Optional[CatalogMetadata]</code> <p>Metadata for the catalog. Defaults to None.</p> <code>None</code> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def __init__(\n    self, dbs: dict[str, Database], metadata: Optional[CatalogMetadata] = None\n):\n    \"\"\"Initialize the Catalog.\n\n    Example usage:\n        ``` py\n        from datarepo.core import Catalog, CatalogMetadata\n        catalog = Catalog(\n            dbs={\n                \"my_db\": ModuleDatabase(my_database_module),\n            },\n            metadata=CatalogMetadata(jupyterhub_url=\"https://jupyterhub.example.com\")\n        )\n        ```\n\n    Args:\n        dbs (dict[str, Database]): A dictionary of database names and their corresponding Database objects.\n        metadata (Optional[CatalogMetadata], optional): Metadata for the catalog. Defaults to None.\n    \"\"\"\n    self._dbs = dbs\n    self._metadata = metadata or CatalogMetadata()\n    self._global_args: dict[str, Any] | None = None\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Catalog.db","title":"<code>db(db_name)</code>","text":"<p>Get a database from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>The name of the database.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If the database is not found.</p> <p>Returns:</p> Name Type Description <code>Database</code> <code>Database</code> <p>The requested database.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def db(self, db_name: str) -&gt; Database:\n    \"\"\"Get a database from the catalog.\n\n    Args:\n        db_name (str): The name of the database.\n\n    Raises:\n        KeyError: If the database is not found.\n\n    Returns:\n        Database: The requested database.\n    \"\"\"\n    db = self._dbs.get(db_name)\n\n    if db is None:\n        raise KeyError(\n            f\"Database '{db_name}' not found. Available databases: {self.dbs()}\"\n        )\n\n    if self._global_args is None:\n        return db\n\n    return DatabaseWithGlobalArgs(db, self._global_args)\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Catalog.dbs","title":"<code>dbs()</code>","text":"<p>Get a list of database names in the catalog.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of database names.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def dbs(self) -&gt; list[str]:\n    \"\"\"Get a list of database names in the catalog.\n\n    Returns:\n        list[str]: A list of database names.\n    \"\"\"\n    return list(self._dbs.keys())\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Catalog.set_global_args","title":"<code>set_global_args(global_args)</code>","text":"<p>Set global arguments for all database queries.</p> <p>Parameters:</p> Name Type Description Default <code>global_args</code> <code>dict[str, Any]</code> <p>A dictionary of global arguments to apply to all database queries.</p> required Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def set_global_args(self, global_args: dict[str, Any]) -&gt; None:\n    \"\"\"Set global arguments for all database queries.\n\n    Args:\n        global_args (dict[str, Any]): A dictionary of global arguments to apply to all database queries.\n    \"\"\"\n    self._global_args = global_args\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.CatalogMetadata","title":"<code>CatalogMetadata</code>  <code>dataclass</code>","text":"<p>Metadata for catalog.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>@dataclass\nclass CatalogMetadata:\n    \"\"\"Metadata for catalog.\"\"\"\n\n    jupyterhub_url: str | None = None\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Database","title":"<code>Database</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for a database that provides access to tables.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>class Database(Protocol):\n    \"\"\"A protocol for a database that provides access to tables.\"\"\"\n\n    def get_tables(self, show_deprecated: bool = False) -&gt; dict[str, TableProtocol]:\n        \"\"\"Get a dictionary of tables in the database.\n\n        Args:\n            show_deprecated (bool, optional): Whether to include deprecated tables. Defaults to False.\n        \"\"\"\n        ...\n\n    def tables(self, show_deprecated: bool = False) -&gt; list[str]:\n        \"\"\"Get a list of table names in the database.\n\n        Args:\n            show_deprecated (bool, optional): Whether to include deprecated tables. Defaults to False.\n\n        Returns:\n            list[str]: A list of table names.\n        \"\"\"\n        return list(self.get_tables(show_deprecated).keys())\n\n    def table(self, name: str, *args: Any, **kwargs: Any) -&gt; NlkDataFrame:\n        \"\"\"Get a table from the database.\n\n        Args:\n            name (str): The name of the table.\n\n        Returns:\n            NlkDataFrame: The requested table.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Database.get_tables","title":"<code>get_tables(show_deprecated=False)</code>","text":"<p>Get a dictionary of tables in the database.</p> <p>Parameters:</p> Name Type Description Default <code>show_deprecated</code> <code>bool</code> <p>Whether to include deprecated tables. Defaults to False.</p> <code>False</code> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def get_tables(self, show_deprecated: bool = False) -&gt; dict[str, TableProtocol]:\n    \"\"\"Get a dictionary of tables in the database.\n\n    Args:\n        show_deprecated (bool, optional): Whether to include deprecated tables. Defaults to False.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Database.table","title":"<code>table(name, *args, **kwargs)</code>","text":"<p>Get a table from the database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table.</p> required <p>Returns:</p> Name Type Description <code>NlkDataFrame</code> <code>NlkDataFrame</code> <p>The requested table.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def table(self, name: str, *args: Any, **kwargs: Any) -&gt; NlkDataFrame:\n    \"\"\"Get a table from the database.\n\n    Args:\n        name (str): The name of the table.\n\n    Returns:\n        NlkDataFrame: The requested table.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.Database.tables","title":"<code>tables(show_deprecated=False)</code>","text":"<p>Get a list of table names in the database.</p> <p>Parameters:</p> Name Type Description Default <code>show_deprecated</code> <code>bool</code> <p>Whether to include deprecated tables. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of table names.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def tables(self, show_deprecated: bool = False) -&gt; list[str]:\n    \"\"\"Get a list of table names in the database.\n\n    Args:\n        show_deprecated (bool, optional): Whether to include deprecated tables. Defaults to False.\n\n    Returns:\n        list[str]: A list of table names.\n    \"\"\"\n    return list(self.get_tables(show_deprecated).keys())\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.ModuleDatabase","title":"<code>ModuleDatabase</code>","text":"<p>               Bases: <code>Database</code></p> <p>A database that is implemented as a Python module.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>class ModuleDatabase(Database):\n    \"\"\"A database that is implemented as a Python module.\"\"\"\n\n    def __init__(self, db: ModuleType) -&gt; None:\n        \"\"\"Initialize the ModuleDatabase.\n\n        Example usage:\n            ``` py\n            import my_database_module\n            db = ModuleDatabase(my_database_module)\n            ```\n\n        Args:\n            db (ModuleType): The database module.\n        \"\"\"\n        self.db = db\n\n    def __getattr__(self, name: str):\n        # HACK: to maintain backwards compatibility when accessing module attributes\n        return self._get_table(name)\n\n    def get_tables(self, show_deprecated: bool = False) -&gt; dict[str, TableProtocol]:\n        \"\"\"Get a dictionary of tables in the database.\n\n        Example usage:\n            ``` py\n            db = ModuleDatabase(my_database_module)\n            tables = db.get_tables(show_deprecated=True)\n            ```\n\n        Args:\n            show_deprecated (bool, optional): Whether to include deprecated tables. Defaults to False.\n\n        Returns:\n            dict[str, TableProtocol]: A dictionary of table names and their corresponding TableProtocol objects.\n        \"\"\"\n        methods = dir(self.db)\n\n        tables = {}\n        for name in methods:\n            table = self._get_table(name)\n            if table is None:\n                continue\n\n            if table.table_metadata.is_deprecated and not show_deprecated:\n                continue\n\n            tables[name] = table\n\n        return tables\n\n    def table(self, name: str, *args: Any, **kwargs: Any) -&gt; NlkDataFrame:\n        \"\"\"Get a table from the database.\n\n        Example usage:\n            ``` py\n            db = ModuleDatabase(my_database_module)\n            table = db.table(\"my_table\")\n            ```\n\n        Args:\n            name (str): The name of the table.\n\n        Raises:\n            KeyError: If the table is not found.\n\n        Returns:\n            NlkDataFrame: The requested table.\n        \"\"\"\n        tbl = self._get_table(name)\n        if tbl is None:\n            raise KeyError(f\"Table '{name}' not found in database\")\n\n        if tbl.table_metadata.is_deprecated:\n            warnings.warn(f\"The table '{name}' is deprecated\", DeprecationWarning)\n\n        return tbl(*args, **kwargs)\n\n    def _get_table(self, name: str) -&gt; TableProtocol | None:\n        \"\"\"Get a table from the database.\n\n        Args:\n            name (str): The name of the table.\n\n        Returns:\n            TableProtocol | None: The requested table or None if not found.\n        \"\"\"\n        table = getattr(self.db, name)\n        if not hasattr(table, \"table_metadata\"):\n            return None\n\n        return cast(TableProtocol, table)\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.ModuleDatabase.__init__","title":"<code>__init__(db)</code>","text":"<p>Initialize the ModuleDatabase.</p> Example usage <pre><code>import my_database_module\ndb = ModuleDatabase(my_database_module)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>ModuleType</code> <p>The database module.</p> required Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def __init__(self, db: ModuleType) -&gt; None:\n    \"\"\"Initialize the ModuleDatabase.\n\n    Example usage:\n        ``` py\n        import my_database_module\n        db = ModuleDatabase(my_database_module)\n        ```\n\n    Args:\n        db (ModuleType): The database module.\n    \"\"\"\n    self.db = db\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.ModuleDatabase.get_tables","title":"<code>get_tables(show_deprecated=False)</code>","text":"<p>Get a dictionary of tables in the database.</p> Example usage <pre><code>db = ModuleDatabase(my_database_module)\ntables = db.get_tables(show_deprecated=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>show_deprecated</code> <code>bool</code> <p>Whether to include deprecated tables. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, TableProtocol]</code> <p>dict[str, TableProtocol]: A dictionary of table names and their corresponding TableProtocol objects.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def get_tables(self, show_deprecated: bool = False) -&gt; dict[str, TableProtocol]:\n    \"\"\"Get a dictionary of tables in the database.\n\n    Example usage:\n        ``` py\n        db = ModuleDatabase(my_database_module)\n        tables = db.get_tables(show_deprecated=True)\n        ```\n\n    Args:\n        show_deprecated (bool, optional): Whether to include deprecated tables. Defaults to False.\n\n    Returns:\n        dict[str, TableProtocol]: A dictionary of table names and their corresponding TableProtocol objects.\n    \"\"\"\n    methods = dir(self.db)\n\n    tables = {}\n    for name in methods:\n        table = self._get_table(name)\n        if table is None:\n            continue\n\n        if table.table_metadata.is_deprecated and not show_deprecated:\n            continue\n\n        tables[name] = table\n\n    return tables\n</code></pre>"},{"location":"api-docs/#datarepo.core.catalog.ModuleDatabase.table","title":"<code>table(name, *args, **kwargs)</code>","text":"<p>Get a table from the database.</p> Example usage <pre><code>db = ModuleDatabase(my_database_module)\ntable = db.table(\"my_table\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If the table is not found.</p> <p>Returns:</p> Name Type Description <code>NlkDataFrame</code> <code>NlkDataFrame</code> <p>The requested table.</p> Source code in <code>src/datarepo/core/catalog/catalog.py</code> <pre><code>def table(self, name: str, *args: Any, **kwargs: Any) -&gt; NlkDataFrame:\n    \"\"\"Get a table from the database.\n\n    Example usage:\n        ``` py\n        db = ModuleDatabase(my_database_module)\n        table = db.table(\"my_table\")\n        ```\n\n    Args:\n        name (str): The name of the table.\n\n    Raises:\n        KeyError: If the table is not found.\n\n    Returns:\n        NlkDataFrame: The requested table.\n    \"\"\"\n    tbl = self._get_table(name)\n    if tbl is None:\n        raise KeyError(f\"Table '{name}' not found in database\")\n\n    if tbl.table_metadata.is_deprecated:\n        warnings.warn(f\"The table '{name}' is deprecated\", DeprecationWarning)\n\n    return tbl(*args, **kwargs)\n</code></pre>"},{"location":"api-docs/#dataframes-module","title":"Dataframes module","text":""},{"location":"api-docs/#tables-module","title":"Tables module","text":""},{"location":"api-docs/#datarepo.core.tables.ClickHouseTable","title":"<code>ClickHouseTable</code>","text":"<p>               Bases: <code>TableProtocol</code></p> <p>A table implementation that reads data from ClickHouse.</p> Source code in <code>src/datarepo/core/tables/clickhouse_table.py</code> <pre><code>class ClickHouseTable(TableProtocol):\n    \"\"\"A table implementation that reads data from ClickHouse.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        schema: pa.Schema,\n        config: ClickHouseTableConfig,\n        description: str = \"\",\n        docs_filters: List[Filter] | None = None,\n        docs_columns: Optional[List[str]] = None,\n        roapi_opts: RoapiOptions | None = None,\n        unique_columns: Optional[List[str]] = None,\n        table_metadata_args: Optional[Dict[str, Any]] = None,\n        stats_cols: Optional[List[str]] = None,\n    ):\n        \"\"\"Initialize the ClickHouseTable.\n\n        Example usage:\n            ```python\n            from datarepo.core.tables import ClickHouseTable, ClickHouseTableConfig\n\n            config = ClickHouseTableConfig(\n                host=\"localhost\",\n                port=8443,\n                username=\"user\",\n                password=\"password\",\n                database=\"default\",\n                secure=True,\n                verify=True,\n                settings={\"max_result_rows\": 1000}\n            )\n\n            table = ClickHouseTable(\n                name=\"my_table\",\n                schema=pa.schema(\n                    [\n                        (\"implant_id\", pa.int64()),\n                        (\"date\", pa.string()),\n                        (\"uniq\", pa.string()),\n                        (\"value\", pa.int64()),\n                    ]\n                ),\n                config=config,\n                description=\"My ClickHouse table\",\n                docs_filters=[...],\n                docs_columns=[...],\n                unique_columns=[\"uniq\"],\n                table_metadata_args={\"answer\": \"42\"},\n                stats_cols=[\"implant_id\"]\n            )\n            ```\n\n        Args:\n            name: Name of the table in ClickHouse\n            schema: Schema of the table\n            config: Configuration for connecting to ClickHouse\n            description: Description of the table for documentation\n            docs_filters: Filters to show in documentation\n            docs_columns: Columns to show in documentation\n            roapi_opts: Options for ROAPI integration\n            unique_columns: Columns to use for deduplication\n            table_metadata_args: Additional metadata arguments\n            stats_cols: Statistics columns, used to define columns that have statistics\n        \"\"\"\n        self.name = name\n        self.schema = schema\n        self.config = config\n        self.unique_columns = unique_columns or []\n        self.docs_filters = docs_filters or []\n        self.docs_columns = docs_columns\n        self.stats_cols = stats_cols or []\n        self.uri = config.get_uri()\n\n        self.table_metadata = TableMetadata(\n            table_type=\"CLICKHOUSE\",\n            description=description,\n            docs_args={\"filters\": self.docs_filters, \"columns\": self.docs_columns},\n            roapi_opts=roapi_opts or RoapiOptions(),\n            **(table_metadata_args or {}),\n        )\n\n    def get_schema(self) -&gt; TableSchema:\n        \"\"\"Generate and return the schema of the table, including columns.\n\n        Returns:\n            TableSchema: table schema containing column information.\n        \"\"\"\n        schema = self.schema\n\n        columns = [\n            TableColumn(\n                column=name,\n                type=str(schema.field(name).type),\n                readonly=False,\n                filter_only=False,\n                has_stats=name in self.stats_cols,\n            )\n            for name in schema.names\n        ]\n\n        return TableSchema(\n            partitions=[],  # Clickhouse does not have partitions exposed in schema.\n            columns=columns,\n        )\n\n    def _build_query(\n        self,\n        filters: InputFilters | None = None,\n        columns: Optional[List[str]] = None,\n    ) -&gt; str:\n        \"\"\"Build a SQL query for the ClickHouse table.\n\n        Args:\n            filters (InputFilters, optional): Filters to apply to the query. Defaults to None.\n            columns (Optional[List[str]], optional): Columns to select in the query. Defaults to None.\n\n        Returns:\n            str: SQL query string to select data from the ClickHouse table.\n        \"\"\"\n        column_expr = \"*\"\n        if columns:\n            valid_columns = [c for c in columns if c in self.schema.names]\n            if valid_columns:\n                column_expr = \", \".join(f\"`{c}`\" for c in valid_columns)\n            else:\n                logger.warning(\n                    f\"No valid columns provided for table {self.name}. Using '*' to select all columns.\"\n                )\n\n        # Build WHERE clause from filters if provided\n        where_clause = \"\"\n        if filters:\n            normalized_filters = normalize_filters(filters)\n            filter_expressions = []\n\n            for filter_set in normalized_filters:\n                set_expressions = []\n                for f in filter_set:\n                    if f.operator == \"=\":\n                        set_expressions.append(\n                            f\"`{f.column}` = {format_value_for_sql(f.value)}\"\n                        )\n                    elif f.operator == \"!=\":\n                        set_expressions.append(\n                            f\"`{f.column}` != {format_value_for_sql(f.value)}\"\n                        )\n                    elif f.operator == \"&gt;\":\n                        set_expressions.append(\n                            f\"`{f.column}` &gt; {format_value_for_sql(f.value)}\"\n                        )\n                    elif f.operator == \"&lt;\":\n                        set_expressions.append(\n                            f\"`{f.column}` &lt; {format_value_for_sql(f.value)}\"\n                        )\n                    elif f.operator == \"&gt;=\":\n                        set_expressions.append(\n                            f\"`{f.column}` &gt;= {format_value_for_sql(f.value)}\"\n                        )\n                    elif f.operator == \"&lt;=\":\n                        set_expressions.append(\n                            f\"`{f.column}` &lt;= {format_value_for_sql(f.value)}\"\n                        )\n                    elif f.operator == \"in\":\n                        values = \", \".join(\n                            format_value_for_sql(v) for v in cast(list, f.value)\n                        )\n                        set_expressions.append(f\"`{f.column}` IN ({values})\")\n                    elif f.operator == \"not in\":\n                        values = \", \".join(\n                            format_value_for_sql(v) for v in cast(list, f.value)\n                        )\n                        set_expressions.append(f\"`{f.column}` NOT IN ({values})\")\n                    elif f.operator in [\n                        \"contains\",\n                        \"includes\",\n                        \"includes any\",\n                        \"includes all\",\n                    ]:\n                        set_expressions.append(\n                            f\"`{f.column}` LIKE {format_value_for_sql(f.value)}\"\n                        )\n\n                if set_expressions:\n                    filter_expressions.append(\"(\" + \" AND \".join(set_expressions) + \")\")\n\n            if filter_expressions:\n                where_clause = \"WHERE \" + \" OR \".join(filter_expressions)\n\n        return f\"SELECT {column_expr} FROM `{self.config.database}`.`{self.name}` {where_clause}\"\n\n    def __call__(  # type: ignore[override]\n        self,\n        filters: InputFilters | None = None,\n        columns: List[str] | None = None,\n        **kwargs: Dict[str, Any],\n    ) -&gt; NlkDataFrame:\n        \"\"\"Query data from the ClickHouse table.\n\n        Example usage:\n            ``` py\n            from datarepo.core.tables import ClickHouseTable, ClickHouseTableConfig\n            config = ClickHouseTableConfig(...)\n            table = ClickHouseTable(...)\n            df = table(filters=[Filter(\"implant_id\", \"=\", 123)], columns=[\"date\", \"value\"])\n            ```\n\n        Args:\n            filters: Filters to apply to the data.\n            columns: Columns to select from the table.\n            **kwargs: Additional arguments.\n\n        Returns:\n            NlkDataFrame: A lazy Polars DataFrame with the requested data.\n        \"\"\"\n        query = self._build_query(filters, columns)\n\n        # use polars read database_uri to read the result of the query\n        df = pl.read_database_uri(\n            query=query,\n            uri=self.uri,\n            engine=\"connectorx\",\n        )\n\n        return df.lazy()\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ClickHouseTable.__call__","title":"<code>__call__(filters=None, columns=None, **kwargs)</code>","text":"<p>Query data from the ClickHouse table.</p> Example usage <pre><code>from datarepo.core.tables import ClickHouseTable, ClickHouseTableConfig\nconfig = ClickHouseTableConfig(...)\ntable = ClickHouseTable(...)\ndf = table(filters=[Filter(\"implant_id\", \"=\", 123)], columns=[\"date\", \"value\"])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>InputFilters | None</code> <p>Filters to apply to the data.</p> <code>None</code> <code>columns</code> <code>List[str] | None</code> <p>Columns to select from the table.</p> <code>None</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>NlkDataFrame</code> <code>NlkDataFrame</code> <p>A lazy Polars DataFrame with the requested data.</p> Source code in <code>src/datarepo/core/tables/clickhouse_table.py</code> <pre><code>def __call__(  # type: ignore[override]\n    self,\n    filters: InputFilters | None = None,\n    columns: List[str] | None = None,\n    **kwargs: Dict[str, Any],\n) -&gt; NlkDataFrame:\n    \"\"\"Query data from the ClickHouse table.\n\n    Example usage:\n        ``` py\n        from datarepo.core.tables import ClickHouseTable, ClickHouseTableConfig\n        config = ClickHouseTableConfig(...)\n        table = ClickHouseTable(...)\n        df = table(filters=[Filter(\"implant_id\", \"=\", 123)], columns=[\"date\", \"value\"])\n        ```\n\n    Args:\n        filters: Filters to apply to the data.\n        columns: Columns to select from the table.\n        **kwargs: Additional arguments.\n\n    Returns:\n        NlkDataFrame: A lazy Polars DataFrame with the requested data.\n    \"\"\"\n    query = self._build_query(filters, columns)\n\n    # use polars read database_uri to read the result of the query\n    df = pl.read_database_uri(\n        query=query,\n        uri=self.uri,\n        engine=\"connectorx\",\n    )\n\n    return df.lazy()\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ClickHouseTable.__init__","title":"<code>__init__(name, schema, config, description='', docs_filters=None, docs_columns=None, roapi_opts=None, unique_columns=None, table_metadata_args=None, stats_cols=None)</code>","text":"<p>Initialize the ClickHouseTable.</p> Example usage <pre><code>from datarepo.core.tables import ClickHouseTable, ClickHouseTableConfig\n\nconfig = ClickHouseTableConfig(\n    host=\"localhost\",\n    port=8443,\n    username=\"user\",\n    password=\"password\",\n    database=\"default\",\n    secure=True,\n    verify=True,\n    settings={\"max_result_rows\": 1000}\n)\n\ntable = ClickHouseTable(\n    name=\"my_table\",\n    schema=pa.schema(\n        [\n            (\"implant_id\", pa.int64()),\n            (\"date\", pa.string()),\n            (\"uniq\", pa.string()),\n            (\"value\", pa.int64()),\n        ]\n    ),\n    config=config,\n    description=\"My ClickHouse table\",\n    docs_filters=[...],\n    docs_columns=[...],\n    unique_columns=[\"uniq\"],\n    table_metadata_args={\"answer\": \"42\"},\n    stats_cols=[\"implant_id\"]\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table in ClickHouse</p> required <code>schema</code> <code>Schema</code> <p>Schema of the table</p> required <code>config</code> <code>ClickHouseTableConfig</code> <p>Configuration for connecting to ClickHouse</p> required <code>description</code> <code>str</code> <p>Description of the table for documentation</p> <code>''</code> <code>docs_filters</code> <code>List[Filter] | None</code> <p>Filters to show in documentation</p> <code>None</code> <code>docs_columns</code> <code>Optional[List[str]]</code> <p>Columns to show in documentation</p> <code>None</code> <code>roapi_opts</code> <code>RoapiOptions | None</code> <p>Options for ROAPI integration</p> <code>None</code> <code>unique_columns</code> <code>Optional[List[str]]</code> <p>Columns to use for deduplication</p> <code>None</code> <code>table_metadata_args</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata arguments</p> <code>None</code> <code>stats_cols</code> <code>Optional[List[str]]</code> <p>Statistics columns, used to define columns that have statistics</p> <code>None</code> Source code in <code>src/datarepo/core/tables/clickhouse_table.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    schema: pa.Schema,\n    config: ClickHouseTableConfig,\n    description: str = \"\",\n    docs_filters: List[Filter] | None = None,\n    docs_columns: Optional[List[str]] = None,\n    roapi_opts: RoapiOptions | None = None,\n    unique_columns: Optional[List[str]] = None,\n    table_metadata_args: Optional[Dict[str, Any]] = None,\n    stats_cols: Optional[List[str]] = None,\n):\n    \"\"\"Initialize the ClickHouseTable.\n\n    Example usage:\n        ```python\n        from datarepo.core.tables import ClickHouseTable, ClickHouseTableConfig\n\n        config = ClickHouseTableConfig(\n            host=\"localhost\",\n            port=8443,\n            username=\"user\",\n            password=\"password\",\n            database=\"default\",\n            secure=True,\n            verify=True,\n            settings={\"max_result_rows\": 1000}\n        )\n\n        table = ClickHouseTable(\n            name=\"my_table\",\n            schema=pa.schema(\n                [\n                    (\"implant_id\", pa.int64()),\n                    (\"date\", pa.string()),\n                    (\"uniq\", pa.string()),\n                    (\"value\", pa.int64()),\n                ]\n            ),\n            config=config,\n            description=\"My ClickHouse table\",\n            docs_filters=[...],\n            docs_columns=[...],\n            unique_columns=[\"uniq\"],\n            table_metadata_args={\"answer\": \"42\"},\n            stats_cols=[\"implant_id\"]\n        )\n        ```\n\n    Args:\n        name: Name of the table in ClickHouse\n        schema: Schema of the table\n        config: Configuration for connecting to ClickHouse\n        description: Description of the table for documentation\n        docs_filters: Filters to show in documentation\n        docs_columns: Columns to show in documentation\n        roapi_opts: Options for ROAPI integration\n        unique_columns: Columns to use for deduplication\n        table_metadata_args: Additional metadata arguments\n        stats_cols: Statistics columns, used to define columns that have statistics\n    \"\"\"\n    self.name = name\n    self.schema = schema\n    self.config = config\n    self.unique_columns = unique_columns or []\n    self.docs_filters = docs_filters or []\n    self.docs_columns = docs_columns\n    self.stats_cols = stats_cols or []\n    self.uri = config.get_uri()\n\n    self.table_metadata = TableMetadata(\n        table_type=\"CLICKHOUSE\",\n        description=description,\n        docs_args={\"filters\": self.docs_filters, \"columns\": self.docs_columns},\n        roapi_opts=roapi_opts or RoapiOptions(),\n        **(table_metadata_args or {}),\n    )\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ClickHouseTable.get_schema","title":"<code>get_schema()</code>","text":"<p>Generate and return the schema of the table, including columns.</p> <p>Returns:</p> Name Type Description <code>TableSchema</code> <code>TableSchema</code> <p>table schema containing column information.</p> Source code in <code>src/datarepo/core/tables/clickhouse_table.py</code> <pre><code>def get_schema(self) -&gt; TableSchema:\n    \"\"\"Generate and return the schema of the table, including columns.\n\n    Returns:\n        TableSchema: table schema containing column information.\n    \"\"\"\n    schema = self.schema\n\n    columns = [\n        TableColumn(\n            column=name,\n            type=str(schema.field(name).type),\n            readonly=False,\n            filter_only=False,\n            has_stats=name in self.stats_cols,\n        )\n        for name in schema.names\n    ]\n\n    return TableSchema(\n        partitions=[],  # Clickhouse does not have partitions exposed in schema.\n        columns=columns,\n    )\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ClickHouseTableConfig","title":"<code>ClickHouseTableConfig</code>  <code>dataclass</code>","text":"<p>Configuration for connecting to ClickHouse.</p> Source code in <code>src/datarepo/core/tables/clickhouse_table.py</code> <pre><code>@dataclass\nclass ClickHouseTableConfig:\n    \"\"\"Configuration for connecting to ClickHouse.\"\"\"\n\n    host: str\n    port: int = 8443\n    username: Optional[str] = None\n    password: Optional[str] = None\n    database: str = \"default\"\n    secure: bool = True\n    verify: bool = True\n    settings: Dict[str, Any] = field(default_factory=dict)\n\n    def get_uri(self) -&gt; str:\n        \"\"\"Construct the URI for the ClickHouse table.\n\n        Returns:\n            str: URI for the ClickHouse table.\n        \"\"\"\n        # check if username and password are provided\n        if not self.username or not self.password:\n            return f\"clickhouse://{self.host}:{self.port}/{self.database}\"\n        return f\"clickhouse://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}\"\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ClickHouseTableConfig.get_uri","title":"<code>get_uri()</code>","text":"<p>Construct the URI for the ClickHouse table.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>URI for the ClickHouse table.</p> Source code in <code>src/datarepo/core/tables/clickhouse_table.py</code> <pre><code>def get_uri(self) -&gt; str:\n    \"\"\"Construct the URI for the ClickHouse table.\n\n    Returns:\n        str: URI for the ClickHouse table.\n    \"\"\"\n    # check if username and password are provided\n    if not self.username or not self.password:\n        return f\"clickhouse://{self.host}:{self.port}/{self.database}\"\n    return f\"clickhouse://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}\"\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltaCacheOptions","title":"<code>DeltaCacheOptions</code>  <code>dataclass</code>","text":"Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>@dataclass\nclass DeltaCacheOptions:\n    # Path to the directory where files are cached. This can be the same for all tables\n    # since the S3 table prefix is included in the cached paths.\n    file_cache_path: str\n    # Duration for which the _last_checkpoint file is cached. A longer duration means\n    # we can reuse cached checkpoints for longer, which could improve loading performance\n    # from not downloading new checkpoint parquets. However, we also need to load more\n    # individual transaction jsons, so this should not be set too high.\n    # A reasonable value for a table with frequent updates (e.g. binned spikes) is 30m.\n    file_cache_last_checkpoint_valid_duration: str | None = None\n\n    def to_storage_options(self) -&gt; dict[str, Any]:\n        \"\"\"Convert the cache options to a dictionary of storage options.\n\n        Returns:\n            dict[str, Any]: A dictionary of storage options that can be used with DeltaTable.\n        \"\"\"\n        opts = {\n            \"file_cache_path\": os.path.expanduser(self.file_cache_path),\n        }\n        if self.file_cache_last_checkpoint_valid_duration is not None:\n            opts[\"file_cache_last_checkpoint_valid_duration\"] = (\n                self.file_cache_last_checkpoint_valid_duration\n            )\n        return opts\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltaCacheOptions.to_storage_options","title":"<code>to_storage_options()</code>","text":"<p>Convert the cache options to a dictionary of storage options.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary of storage options that can be used with DeltaTable.</p> Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>def to_storage_options(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the cache options to a dictionary of storage options.\n\n    Returns:\n        dict[str, Any]: A dictionary of storage options that can be used with DeltaTable.\n    \"\"\"\n    opts = {\n        \"file_cache_path\": os.path.expanduser(self.file_cache_path),\n    }\n    if self.file_cache_last_checkpoint_valid_duration is not None:\n        opts[\"file_cache_last_checkpoint_valid_duration\"] = (\n            self.file_cache_last_checkpoint_valid_duration\n        )\n    return opts\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltalakeTable","title":"<code>DeltalakeTable</code>","text":"<p>               Bases: <code>TableProtocol</code></p> <p>A table that is backed by a Delta Lake table.</p> Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>class DeltalakeTable(TableProtocol):\n    \"\"\"A table that is backed by a Delta Lake table.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        uri: str,\n        schema: pa.Schema,\n        description: str = \"\",\n        docs_filters: list[Filter] = [],\n        docs_columns: list[str] | None = None,\n        roapi_opts: RoapiOptions | None = None,\n        unique_columns: list[str] | None = None,\n        table_metadata_args: dict[str, Any] | None = None,\n        stats_cols: list[str] | None = None,\n        extra_cols: list[tuple[pl.Expr, str]] | None = None,\n    ):\n        \"\"\"Initialize the DeltalakeTable.\n\n        Args:\n            name (str): table name, used as the table identifier in the DeltaTable\n            uri (str): uri of the table, e.g. \"s3://bucket/path/to/table\"\n            schema (pa.Schema): schema of the table, used to define the table structure\n            description (str, optional): description of the table, used for documentation. Defaults to \"\".\n            docs_filters (list[Filter], optional): documentation filters, used to filter the table in the documentation. Defaults to [].\n            docs_columns (list[str] | None, optional): documentation columns, used to define the columns in the documentation. Defaults to None.\n            roapi_opts (RoapiOptions | None, optional): ROAPI options, used to configure the ROAPI for the table. Defaults to None.\n            unique_columns (list[str] | None, optional): unique columns in the table, used to optimize the read performance. Defaults to None.\n            table_metadata_args (dict[str, Any] | None, optional): table metadata arguments, used to configure the table metadata. Defaults to None.\n            stats_cols (list[str] | None, optional): statistics columns, used to define the columns that have statistics. Defaults to None.\n            extra_cols (list[tuple[pl.Expr, str]] | None, optional): extra columns to add to the table, where each tuple contains a Polars expression and its type annotation. Defaults to None.\n        \"\"\"\n        self.name = name\n        self.uri = uri\n        self.schema = schema\n        self.unique_columns = unique_columns\n        self.stats_cols = stats_cols or []\n        self.extra_cols = extra_cols or []\n\n        self.table_metadata = TableMetadata(\n            table_type=\"DELTA_LAKE\",\n            description=description,\n            docs_args={\"filters\": docs_filters, \"columns\": docs_columns},\n            roapi_opts=roapi_opts or DeltaRoapiOptions(),\n            **(table_metadata_args or {}),\n        )\n\n    def get_schema(self) -&gt; TableSchema:\n        \"\"\"Generate and return the schema of the table, including partitions and columns.\n\n        Returns:\n            TableSchema: table schema containing partition and column information.\n        \"\"\"\n        dt = self.delta_table()\n        schema = self.schema\n        partition_cols = dt.metadata().partition_columns\n        filters = {\n            f.column: f.value\n            for f in self.table_metadata.docs_args.get(\"filters\", [])\n            if isinstance(f, Filter)\n        }\n        partitions = [\n            TablePartition(\n                column_name=col,\n                type_annotation=str(schema.field(col).type),\n                value=filters.get(col),\n            )\n            for col in partition_cols\n        ]\n        columns = [\n            TableColumn(\n                column=name,\n                type=str(schema.field(name).type),\n                readonly=False,\n                filter_only=False,\n                has_stats=name in partition_cols or name in self.stats_cols,\n            )\n            for name in schema.names\n        ]\n        columns += [\n            TableColumn(\n                column=expr.meta.output_name(),\n                type=expr_type,\n                readonly=True,\n                filter_only=False,\n                has_stats=False,\n            )\n            for expr, expr_type in self.extra_cols\n        ]\n        return TableSchema(\n            partitions=partitions,\n            columns=columns,\n        )\n\n    def __call__(\n        self,\n        filters: DeltaInputFilters | None = None,\n        columns: list[str] | None = None,\n        boto3_session: boto3.Session | None = None,\n        endpoint_url: str | None = None,\n        timeout: str | None = None,\n        cache_options: DeltaCacheOptions | None = None,\n        **kwargs: Any,\n    ) -&gt; NlkDataFrame:\n        \"\"\"Fetch a dataframe from the Delta Lake table.\n\n        Args:\n            filters (DeltaInputFilters | None, optional): filters to apply to the table. Defaults to None.\n            columns (list[str] | None, optional): columns to select from the table. Defaults to None.\n            boto3_session (boto3.Session | None, optional): boto3 session to use for S3 access. Defaults to None.\n            endpoint_url (str | None, optional): endpoint URL for S3 access. Defaults to None.\n            timeout (str | None, optional): timeout for S3 access. Defaults to None.\n            cache_options (DeltaCacheOptions | None, optional): cache options for the Delta Lake table. Defaults to None.\n\n        Returns:\n            NlkDataFrame: a dataframe containing the data from the Delta Lake table, filtered and selected according to the provided parameters.\n        \"\"\"\n        storage_options = {\n            \"timeout\": timeout or DEFAULT_TIMEOUT,\n            **get_storage_options(\n                boto3_session=boto3_session, endpoint_url=endpoint_url\n            ),\n        }\n        if cache_options is not None:\n            storage_options = {\n                **storage_options,\n                **cache_options.to_storage_options(),\n            }\n        dt = self.delta_table(storage_options=storage_options)\n\n        return self.construct_df(dt=dt, filters=filters, columns=columns)\n\n    def construct_df(\n        self,\n        dt: DeltaTable,\n        filters: DeltaInputFilters | None = None,\n        columns: list[str] | None = None,\n    ) -&gt; NlkDataFrame:\n        \"\"\"Construct a dataframe from the Delta Lake table.\n\n        Args:\n            dt (DeltaTable): The DeltaTable object representing the Delta Lake table.\n            filters (DeltaInputFilters | None, optional): filters to apply to the table. Defaults to None.\n            columns (list[str] | None, optional): columns to select from the table. Defaults to None.\n\n        Returns:\n            NlkDataFrame: a dataframe containing the data from the Delta Lake table, filtered and selected according to the provided parameters.\n        \"\"\"\n        # Use schema defined on this table, the physical schema in deltalake metadata might be different\n        schema = self.schema\n\n        predicate_str = datafusion_predicate_from_filters(schema, filters)\n\n        # These should not be read because they don't exist in the delta table\n        extra_col_exprs = [expr for expr, _ in self.extra_cols]\n        extra_column_names = set(expr.meta.output_name() for expr in extra_col_exprs)\n\n        columns_to_read = None\n        unique_column_names = set(self.unique_columns or [])\n        if columns:\n            columns_to_read = list(\n                (set(columns) | unique_column_names) - extra_column_names\n            )\n\n        # TODO(peter): consider a sql builder for more complex queries?\n        select_cols = (\n            \", \".join([f'\"{col}\"' for col in columns_to_read])\n            if columns_to_read\n            else \"*\"\n        )\n        condition = f\"WHERE {predicate_str}\" if predicate_str else \"\"\n        query_string = f\"\"\"\n            SELECT {select_cols}\n            FROM \"{self.name}\"\n            {condition}\n        \"\"\"\n        with warnings.catch_warnings():\n            # Ignore ExperimentalWarning emitted from QueryBuilder\n            warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n            batches = (\n                QueryBuilder().register(self.name, dt).execute(query_string).fetchall()\n            )\n\n        # Since we might cast unique string columns to categoricals, use a string cache to\n        # improve performance when combining multiple dataframes\n        with pl.StringCache():\n            if batches:\n                frame = pl.from_arrow(batches, rechunk=False)\n                if isinstance(frame, pl.Series):\n                    frame = frame.to_frame()\n                frame = _normalize_df(frame, self.schema, columns=columns_to_read)\n            else:\n                # If dataset is empty, the returned dataframe will have no columns\n                frame = _empty_normalized_df(schema)\n\n            if self.extra_cols:\n                frame = frame.with_columns(extra_col_exprs)\n\n            if self.unique_columns:\n                # Cast unique string columns to categoricals first\n                # In some cases, this reduces peak memory usage up to 50%\n                curr_schema = frame.schema\n                cat_schema = {\n                    col: pl.Categorical\n                    for col in curr_schema\n                    if curr_schema[col] == pl.String\n                }\n                frame = (\n                    frame.cast(cat_schema)  # type: ignore[arg-type]\n                    .unique(subset=self.unique_columns, maintain_order=True)\n                    .cast(curr_schema)  # type: ignore[arg-type]\n                )\n\n        if columns:\n            frame = frame.select(columns)\n\n        return frame.lazy()\n\n    def delta_table(\n        self, storage_options: dict[str, Any] | None = None, version: int | None = None\n    ) -&gt; DeltaTable:\n        \"\"\"Get the DeltaTable object for this table.\n\n        Args:\n            storage_options (dict[str, Any] | None, optional): Storage options for the DeltaTable, such as S3 access credentials. Defaults to None.\n            version (int | None, optional): Version of the Delta table to read. If None, the latest version is used. Defaults to None.\n\n        Returns:\n            DeltaTable: The DeltaTable object representing the Delta Lake table.\n        \"\"\"\n        return DeltaTable(\n            table_uri=self.uri, storage_options=storage_options, version=version\n        )\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltalakeTable.__call__","title":"<code>__call__(filters=None, columns=None, boto3_session=None, endpoint_url=None, timeout=None, cache_options=None, **kwargs)</code>","text":"<p>Fetch a dataframe from the Delta Lake table.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>DeltaInputFilters | None</code> <p>filters to apply to the table. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>columns to select from the table. Defaults to None.</p> <code>None</code> <code>boto3_session</code> <code>Session | None</code> <p>boto3 session to use for S3 access. Defaults to None.</p> <code>None</code> <code>endpoint_url</code> <code>str | None</code> <p>endpoint URL for S3 access. Defaults to None.</p> <code>None</code> <code>timeout</code> <code>str | None</code> <p>timeout for S3 access. Defaults to None.</p> <code>None</code> <code>cache_options</code> <code>DeltaCacheOptions | None</code> <p>cache options for the Delta Lake table. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NlkDataFrame</code> <code>NlkDataFrame</code> <p>a dataframe containing the data from the Delta Lake table, filtered and selected according to the provided parameters.</p> Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>def __call__(\n    self,\n    filters: DeltaInputFilters | None = None,\n    columns: list[str] | None = None,\n    boto3_session: boto3.Session | None = None,\n    endpoint_url: str | None = None,\n    timeout: str | None = None,\n    cache_options: DeltaCacheOptions | None = None,\n    **kwargs: Any,\n) -&gt; NlkDataFrame:\n    \"\"\"Fetch a dataframe from the Delta Lake table.\n\n    Args:\n        filters (DeltaInputFilters | None, optional): filters to apply to the table. Defaults to None.\n        columns (list[str] | None, optional): columns to select from the table. Defaults to None.\n        boto3_session (boto3.Session | None, optional): boto3 session to use for S3 access. Defaults to None.\n        endpoint_url (str | None, optional): endpoint URL for S3 access. Defaults to None.\n        timeout (str | None, optional): timeout for S3 access. Defaults to None.\n        cache_options (DeltaCacheOptions | None, optional): cache options for the Delta Lake table. Defaults to None.\n\n    Returns:\n        NlkDataFrame: a dataframe containing the data from the Delta Lake table, filtered and selected according to the provided parameters.\n    \"\"\"\n    storage_options = {\n        \"timeout\": timeout or DEFAULT_TIMEOUT,\n        **get_storage_options(\n            boto3_session=boto3_session, endpoint_url=endpoint_url\n        ),\n    }\n    if cache_options is not None:\n        storage_options = {\n            **storage_options,\n            **cache_options.to_storage_options(),\n        }\n    dt = self.delta_table(storage_options=storage_options)\n\n    return self.construct_df(dt=dt, filters=filters, columns=columns)\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltalakeTable.__init__","title":"<code>__init__(name, uri, schema, description='', docs_filters=[], docs_columns=None, roapi_opts=None, unique_columns=None, table_metadata_args=None, stats_cols=None, extra_cols=None)</code>","text":"<p>Initialize the DeltalakeTable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>table name, used as the table identifier in the DeltaTable</p> required <code>uri</code> <code>str</code> <p>uri of the table, e.g. \"s3://bucket/path/to/table\"</p> required <code>schema</code> <code>Schema</code> <p>schema of the table, used to define the table structure</p> required <code>description</code> <code>str</code> <p>description of the table, used for documentation. Defaults to \"\".</p> <code>''</code> <code>docs_filters</code> <code>list[Filter]</code> <p>documentation filters, used to filter the table in the documentation. Defaults to [].</p> <code>[]</code> <code>docs_columns</code> <code>list[str] | None</code> <p>documentation columns, used to define the columns in the documentation. Defaults to None.</p> <code>None</code> <code>roapi_opts</code> <code>RoapiOptions | None</code> <p>ROAPI options, used to configure the ROAPI for the table. Defaults to None.</p> <code>None</code> <code>unique_columns</code> <code>list[str] | None</code> <p>unique columns in the table, used to optimize the read performance. Defaults to None.</p> <code>None</code> <code>table_metadata_args</code> <code>dict[str, Any] | None</code> <p>table metadata arguments, used to configure the table metadata. Defaults to None.</p> <code>None</code> <code>stats_cols</code> <code>list[str] | None</code> <p>statistics columns, used to define the columns that have statistics. Defaults to None.</p> <code>None</code> <code>extra_cols</code> <code>list[tuple[Expr, str]] | None</code> <p>extra columns to add to the table, where each tuple contains a Polars expression and its type annotation. Defaults to None.</p> <code>None</code> Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    uri: str,\n    schema: pa.Schema,\n    description: str = \"\",\n    docs_filters: list[Filter] = [],\n    docs_columns: list[str] | None = None,\n    roapi_opts: RoapiOptions | None = None,\n    unique_columns: list[str] | None = None,\n    table_metadata_args: dict[str, Any] | None = None,\n    stats_cols: list[str] | None = None,\n    extra_cols: list[tuple[pl.Expr, str]] | None = None,\n):\n    \"\"\"Initialize the DeltalakeTable.\n\n    Args:\n        name (str): table name, used as the table identifier in the DeltaTable\n        uri (str): uri of the table, e.g. \"s3://bucket/path/to/table\"\n        schema (pa.Schema): schema of the table, used to define the table structure\n        description (str, optional): description of the table, used for documentation. Defaults to \"\".\n        docs_filters (list[Filter], optional): documentation filters, used to filter the table in the documentation. Defaults to [].\n        docs_columns (list[str] | None, optional): documentation columns, used to define the columns in the documentation. Defaults to None.\n        roapi_opts (RoapiOptions | None, optional): ROAPI options, used to configure the ROAPI for the table. Defaults to None.\n        unique_columns (list[str] | None, optional): unique columns in the table, used to optimize the read performance. Defaults to None.\n        table_metadata_args (dict[str, Any] | None, optional): table metadata arguments, used to configure the table metadata. Defaults to None.\n        stats_cols (list[str] | None, optional): statistics columns, used to define the columns that have statistics. Defaults to None.\n        extra_cols (list[tuple[pl.Expr, str]] | None, optional): extra columns to add to the table, where each tuple contains a Polars expression and its type annotation. Defaults to None.\n    \"\"\"\n    self.name = name\n    self.uri = uri\n    self.schema = schema\n    self.unique_columns = unique_columns\n    self.stats_cols = stats_cols or []\n    self.extra_cols = extra_cols or []\n\n    self.table_metadata = TableMetadata(\n        table_type=\"DELTA_LAKE\",\n        description=description,\n        docs_args={\"filters\": docs_filters, \"columns\": docs_columns},\n        roapi_opts=roapi_opts or DeltaRoapiOptions(),\n        **(table_metadata_args or {}),\n    )\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltalakeTable.construct_df","title":"<code>construct_df(dt, filters=None, columns=None)</code>","text":"<p>Construct a dataframe from the Delta Lake table.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>DeltaTable</code> <p>The DeltaTable object representing the Delta Lake table.</p> required <code>filters</code> <code>DeltaInputFilters | None</code> <p>filters to apply to the table. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>columns to select from the table. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NlkDataFrame</code> <code>NlkDataFrame</code> <p>a dataframe containing the data from the Delta Lake table, filtered and selected according to the provided parameters.</p> Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>def construct_df(\n    self,\n    dt: DeltaTable,\n    filters: DeltaInputFilters | None = None,\n    columns: list[str] | None = None,\n) -&gt; NlkDataFrame:\n    \"\"\"Construct a dataframe from the Delta Lake table.\n\n    Args:\n        dt (DeltaTable): The DeltaTable object representing the Delta Lake table.\n        filters (DeltaInputFilters | None, optional): filters to apply to the table. Defaults to None.\n        columns (list[str] | None, optional): columns to select from the table. Defaults to None.\n\n    Returns:\n        NlkDataFrame: a dataframe containing the data from the Delta Lake table, filtered and selected according to the provided parameters.\n    \"\"\"\n    # Use schema defined on this table, the physical schema in deltalake metadata might be different\n    schema = self.schema\n\n    predicate_str = datafusion_predicate_from_filters(schema, filters)\n\n    # These should not be read because they don't exist in the delta table\n    extra_col_exprs = [expr for expr, _ in self.extra_cols]\n    extra_column_names = set(expr.meta.output_name() for expr in extra_col_exprs)\n\n    columns_to_read = None\n    unique_column_names = set(self.unique_columns or [])\n    if columns:\n        columns_to_read = list(\n            (set(columns) | unique_column_names) - extra_column_names\n        )\n\n    # TODO(peter): consider a sql builder for more complex queries?\n    select_cols = (\n        \", \".join([f'\"{col}\"' for col in columns_to_read])\n        if columns_to_read\n        else \"*\"\n    )\n    condition = f\"WHERE {predicate_str}\" if predicate_str else \"\"\n    query_string = f\"\"\"\n        SELECT {select_cols}\n        FROM \"{self.name}\"\n        {condition}\n    \"\"\"\n    with warnings.catch_warnings():\n        # Ignore ExperimentalWarning emitted from QueryBuilder\n        warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n        batches = (\n            QueryBuilder().register(self.name, dt).execute(query_string).fetchall()\n        )\n\n    # Since we might cast unique string columns to categoricals, use a string cache to\n    # improve performance when combining multiple dataframes\n    with pl.StringCache():\n        if batches:\n            frame = pl.from_arrow(batches, rechunk=False)\n            if isinstance(frame, pl.Series):\n                frame = frame.to_frame()\n            frame = _normalize_df(frame, self.schema, columns=columns_to_read)\n        else:\n            # If dataset is empty, the returned dataframe will have no columns\n            frame = _empty_normalized_df(schema)\n\n        if self.extra_cols:\n            frame = frame.with_columns(extra_col_exprs)\n\n        if self.unique_columns:\n            # Cast unique string columns to categoricals first\n            # In some cases, this reduces peak memory usage up to 50%\n            curr_schema = frame.schema\n            cat_schema = {\n                col: pl.Categorical\n                for col in curr_schema\n                if curr_schema[col] == pl.String\n            }\n            frame = (\n                frame.cast(cat_schema)  # type: ignore[arg-type]\n                .unique(subset=self.unique_columns, maintain_order=True)\n                .cast(curr_schema)  # type: ignore[arg-type]\n            )\n\n    if columns:\n        frame = frame.select(columns)\n\n    return frame.lazy()\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltalakeTable.delta_table","title":"<code>delta_table(storage_options=None, version=None)</code>","text":"<p>Get the DeltaTable object for this table.</p> <p>Parameters:</p> Name Type Description Default <code>storage_options</code> <code>dict[str, Any] | None</code> <p>Storage options for the DeltaTable, such as S3 access credentials. Defaults to None.</p> <code>None</code> <code>version</code> <code>int | None</code> <p>Version of the Delta table to read. If None, the latest version is used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DeltaTable</code> <code>DeltaTable</code> <p>The DeltaTable object representing the Delta Lake table.</p> Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>def delta_table(\n    self, storage_options: dict[str, Any] | None = None, version: int | None = None\n) -&gt; DeltaTable:\n    \"\"\"Get the DeltaTable object for this table.\n\n    Args:\n        storage_options (dict[str, Any] | None, optional): Storage options for the DeltaTable, such as S3 access credentials. Defaults to None.\n        version (int | None, optional): Version of the Delta table to read. If None, the latest version is used. Defaults to None.\n\n    Returns:\n        DeltaTable: The DeltaTable object representing the Delta Lake table.\n    \"\"\"\n    return DeltaTable(\n        table_uri=self.uri, storage_options=storage_options, version=version\n    )\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.DeltalakeTable.get_schema","title":"<code>get_schema()</code>","text":"<p>Generate and return the schema of the table, including partitions and columns.</p> <p>Returns:</p> Name Type Description <code>TableSchema</code> <code>TableSchema</code> <p>table schema containing partition and column information.</p> Source code in <code>src/datarepo/core/tables/deltalake_table.py</code> <pre><code>def get_schema(self) -&gt; TableSchema:\n    \"\"\"Generate and return the schema of the table, including partitions and columns.\n\n    Returns:\n        TableSchema: table schema containing partition and column information.\n    \"\"\"\n    dt = self.delta_table()\n    schema = self.schema\n    partition_cols = dt.metadata().partition_columns\n    filters = {\n        f.column: f.value\n        for f in self.table_metadata.docs_args.get(\"filters\", [])\n        if isinstance(f, Filter)\n    }\n    partitions = [\n        TablePartition(\n            column_name=col,\n            type_annotation=str(schema.field(col).type),\n            value=filters.get(col),\n        )\n        for col in partition_cols\n    ]\n    columns = [\n        TableColumn(\n            column=name,\n            type=str(schema.field(name).type),\n            readonly=False,\n            filter_only=False,\n            has_stats=name in partition_cols or name in self.stats_cols,\n        )\n        for name in schema.names\n    ]\n    columns += [\n        TableColumn(\n            column=expr.meta.output_name(),\n            type=expr_type,\n            readonly=True,\n            filter_only=False,\n            has_stats=False,\n        )\n        for expr, expr_type in self.extra_cols\n    ]\n    return TableSchema(\n        partitions=partitions,\n        columns=columns,\n    )\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.Filter","title":"<code>Filter</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Filter represents a condition to be applied to a column in a table.</p> Source code in <code>src/datarepo/core/tables/filters.py</code> <pre><code>class Filter(NamedTuple):\n    \"\"\"Filter represents a condition to be applied to a column in a table.\"\"\"\n\n    column: str\n    operator: FilterOperator\n    value: Any\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ParquetTable","title":"<code>ParquetTable</code>","text":"<p>               Bases: <code>TableProtocol</code></p> <p>A table that is stored in Parquet format.</p> Source code in <code>src/datarepo/core/tables/parquet_table.py</code> <pre><code>class ParquetTable(TableProtocol):\n    \"\"\"A table that is stored in Parquet format.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        uri: str,\n        partitioning: list[Partition],\n        partitioning_scheme: PartitioningScheme = PartitioningScheme.DIRECTORY,\n        description: str = \"\",\n        docs_filters: list[Filter] = [],\n        docs_columns: list[str] | None = None,\n        roapi_opts: RoapiOptions | None = None,\n        parquet_file_name: str = \"df.parquet\",\n        table_metadata_args: dict[str, Any] | None = None,\n    ):\n        \"\"\"Initialize the ParquetTable.\n\n        Args:\n            name (str): name of the table, used for documentation and metadata.\n            uri (str): uri of the table, typically an S3 bucket path.\n            partitioning (list[Partition]): partitioning scheme for the table.\n                This is a list of Partition objects, which define the columns and types used for partitioning\n            partitioning_scheme (PartitioningScheme, optional): scheme used for partitioning.\n                Defaults to PartitioningScheme.DIRECTORY.\n            description (str, optional): description of the table, used for documentation.\n                Defaults to \"\".\n            docs_filters (list[Filter], optional): documentation filters for the table.\n                These filters are used to generate documentation and are not applied to the data.\n                Defaults to [].\n            docs_columns (list[str] | None, optional): docsumentation columns for the table.\n                These columns are used to generate documentation and are not applied to the data.\n                Defaults to None.\n            roapi_opts (RoapiOptions | None, optional): Read-only API options for the table.\n                These options are used to configure the ROAPI endpoint for the table.\n                Defaults to None.\n            parquet_file_name (str, optional): parquet file name to use when building file fragments.\n            table_metadata_args (dict[str, Any] | None, optional): additional metadata arguments for the table.\n\n        Raises:\n            ValueError: if the partitioning_scheme is not a valid PartitioningScheme.\n        \"\"\"\n        if not isinstance(partitioning_scheme, PartitioningScheme):\n            raise ValueError(f\"Invalid partitioning scheme, got {partitioning_scheme}\")\n\n        self.name = name\n        self.uri = uri\n        self.partitioning = partitioning\n        self.partitioning_scheme = partitioning_scheme\n\n        self.table_metadata = TableMetadata(\n            table_type=\"PARQUET\",\n            description=description,\n            docs_args={\"filters\": docs_filters, \"columns\": docs_columns},\n            roapi_opts=roapi_opts,\n            **(table_metadata_args or {}),\n        )\n\n        self.parquet_file_name = parquet_file_name\n\n    def get_schema(self) -&gt; TableSchema:\n        \"\"\"Generates the schema of the table, including partitions and columns.\n\n        Returns:\n            TableSchema: table schema containing partitions and columns.\n        \"\"\"\n        partitions = [\n            TablePartition(\n                column_name=filter.column,\n                type_annotation=type(filter.value).__name__,\n                value=filter.value,\n            )\n            for filter in self.table_metadata.docs_args.get(\"filters\", [])\n        ]\n\n        # Pop the selected columns so that we still pull the full schema below\n        docs_args = {**self.table_metadata.docs_args}\n        docs_args.pop(\"columns\")\n\n        columns = []\n        if docs_args or not partitions:\n            table: NlkDataFrame = self(**docs_args)\n            columns = [\n                TableColumn(\n                    column=key,\n                    type=type.__str__(),\n                    readonly=False,\n                    filter_only=False,\n                    has_stats=False,\n                )\n                for key, type in table.schema.items()\n            ]\n\n        return TableSchema(partitions=partitions, columns=columns)\n\n    def __call__(\n        self,\n        filters: InputFilters | None = None,\n        columns: Optional[list[str]] = None,\n        boto3_session: boto3.Session | None = None,\n        endpoint_url: str | None = None,\n        **kwargs: Any,\n    ) -&gt; NlkDataFrame:\n        \"\"\"Fetches data from the Parquet table based on the provided filters and columns.\n\n        Args:\n            filters (InputFilters | None, optional): filters to apply to the data. Defaults to None.\n            columns (Optional[list[str]], optional): columns to select from the data. Defaults to None.\n            boto3_session (boto3.Session | None, optional): boto3 session to use for S3 access. Defaults to None.\n            endpoint_url (str | None, optional): endpoint URL for S3 access. Defaults to None.\n\n        Returns:\n            NlkDataFrame: A DataFrame containing the filtered data from the Parquet table.\n        \"\"\"\n        normalized_filters = normalize_filters(filters)\n        (\n            uri,\n            remaining_partitions,\n            remaining_filters,\n            applied_filters,\n        ) = self._build_uri_from_filters(normalized_filters)\n\n        storage_options = get_storage_options(\n            boto3_session=boto3_session,\n            endpoint_url=endpoint_url,\n        )\n\n        df = pl.scan_parquet(\n            uri,\n            hive_partitioning=len(remaining_partitions) &gt; 0,\n            hive_schema={\n                partition.column: partition.col_type\n                for partition in remaining_partitions\n            },\n            allow_missing_columns=True,\n            storage_options=storage_options,\n        )\n\n        if applied_filters:\n            # Add columns removed from partitions and added to uri\n            df = df.with_columns(\n                pl.lit(f.value)\n                .cast(\n                    next(\n                        partition.col_type\n                        for partition in self.partitioning\n                        if partition.column == f.column\n                    )\n                )\n                .alias(f.column)\n                for f in applied_filters\n            )\n\n        if remaining_filters:\n            filter_expr = _filters_to_expr(remaining_filters)\n            if filter_expr is not None:\n                df = df.filter(filter_expr)\n\n        if columns:\n            df = df.select(columns)\n\n        return df\n\n    def build_file_fragment(self, filters: list[Filter]) -&gt; str:\n        \"\"\"\n        Returns a file path from the base table URI with the given filters.\n        This will raise an error if the filter does not specify all partitions.\n\n        This is currently used to generate the file path used by ROAPI to infer schemas.\n        \"\"\"\n        uri_with_prefix, partitions, _, _ = self._build_uri_from_filters(\n            normalize_filters(filters), include_base_uri=False\n        )\n        if len(partitions) &gt; 0:\n            partition_names = [partition.column for partition in partitions]\n            raise ValueError(\n                f\"Not enough partitions specified, missing: {partition_names}\"\n            )\n\n        return path.join(uri_with_prefix, self.parquet_file_name)\n\n    def _build_uri_from_filters(\n        self,\n        filters: NormalizedFilters,\n        include_base_uri: bool = True,\n    ) -&gt; tuple[str, list[Partition], NormalizedFilters, list[Filter]]:\n        \"\"\"Attempts to build an S3 list prefix from the given filters.\n        We do this because pyarrow runs an S3 List() query on the base URI\n        before applying filters to possible files. This can be slow.\n\n        We can force pyarrow to do more sophisticated prefix filtering\n        if we pre-construct the URI before making the call. If a partition\n        has exactly one filter that uses strict equality, we know that\n        it will contain that filter string in the URI. We attempt to do this\n        for each partition filter, in order, until we encounter a\n        partition that:\n            1. does not have a filter, or\n            2. has a more than one filters, or\n            3. has filters that are not strict equality checks.\n\n        This gives us the longest URL that can be used as a prefix filter for\n        all files returned by the S3 List() call.\n\n        In a benchmark, this brought reading ~1 million rows of binned spike files\n        from 12s to 1.5s.\n\n        In the future, we can further optimize this by building a list of\n        candidate URIs and doing separate prefix filtering with each of those, in parallel.\n\n        Long-term we should push this logic into pyarrow and make an upstream commit.\n\n        NOTE: Add trailing slash - this is important to ensure that,\n        when partitioning only by implant ID, a future 5-digit implant beginning with the same 4 digits\n        as a 4-digit implant is not included in a 4-digit implant's query.\n        \"\"\"\n        uri = self.uri if include_base_uri else \"\"\n\n        if not filters or not self.partitioning:\n            return uri, self.partitioning, filters, []\n\n        partitions = list(self.partitioning)\n        filters = [list(filter_set) for filter_set in filters]\n        applied_filters = []\n\n        for i, partition in enumerate(self.partitioning):\n            partition_filters = [\n                exactly_one_equality_filter(partition, f) for f in filters\n            ]\n\n            if len(partition_filters) == 0:\n                break\n\n            # either 0 or multiple filters for this partition,\n            # break and deal with the s3 list() query\n            if any(partition_filter is None for partition_filter in partition_filters):\n                break\n\n            # Only move forward if all remaining filter sets have the same partition filter\n            if not all(\n                partition_filter == partition_filters[0]\n                for partition_filter in partition_filters\n            ):\n                break\n\n            partition_filter = partition_filters[0]\n\n            if partition_filter is None:\n                continue\n\n            if self.partitioning_scheme == PartitioningScheme.DIRECTORY:\n                partition_component = str(partition_filter.value)\n            elif self.partitioning_scheme == PartitioningScheme.HIVE:\n                partition_component = (\n                    partition.column + \"=\" + str(partition_filter.value)\n                )\n\n            uri = path.join(uri, partition_component)\n\n            # remove the partition and filter since it's been applied\n            # technically might not need to remove the partitions,\n            # but they are semantically meaningless as we already have\n            # constructed the URI, so we can pop them\n            partitions.remove(partition)\n            for filter_set in filters:\n                filter_set.remove(partition_filter)\n\n            applied_filters.append(partition_filter)\n\n        uri = path.join(\n            uri, \"\"\n        )  # trailing slash prevents inclusion of partitions that are subsets of other partitions\n\n        return (uri, partitions, filters, applied_filters)\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ParquetTable.__call__","title":"<code>__call__(filters=None, columns=None, boto3_session=None, endpoint_url=None, **kwargs)</code>","text":"<p>Fetches data from the Parquet table based on the provided filters and columns.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>InputFilters | None</code> <p>filters to apply to the data. Defaults to None.</p> <code>None</code> <code>columns</code> <code>Optional[list[str]]</code> <p>columns to select from the data. Defaults to None.</p> <code>None</code> <code>boto3_session</code> <code>Session | None</code> <p>boto3 session to use for S3 access. Defaults to None.</p> <code>None</code> <code>endpoint_url</code> <code>str | None</code> <p>endpoint URL for S3 access. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NlkDataFrame</code> <code>NlkDataFrame</code> <p>A DataFrame containing the filtered data from the Parquet table.</p> Source code in <code>src/datarepo/core/tables/parquet_table.py</code> <pre><code>def __call__(\n    self,\n    filters: InputFilters | None = None,\n    columns: Optional[list[str]] = None,\n    boto3_session: boto3.Session | None = None,\n    endpoint_url: str | None = None,\n    **kwargs: Any,\n) -&gt; NlkDataFrame:\n    \"\"\"Fetches data from the Parquet table based on the provided filters and columns.\n\n    Args:\n        filters (InputFilters | None, optional): filters to apply to the data. Defaults to None.\n        columns (Optional[list[str]], optional): columns to select from the data. Defaults to None.\n        boto3_session (boto3.Session | None, optional): boto3 session to use for S3 access. Defaults to None.\n        endpoint_url (str | None, optional): endpoint URL for S3 access. Defaults to None.\n\n    Returns:\n        NlkDataFrame: A DataFrame containing the filtered data from the Parquet table.\n    \"\"\"\n    normalized_filters = normalize_filters(filters)\n    (\n        uri,\n        remaining_partitions,\n        remaining_filters,\n        applied_filters,\n    ) = self._build_uri_from_filters(normalized_filters)\n\n    storage_options = get_storage_options(\n        boto3_session=boto3_session,\n        endpoint_url=endpoint_url,\n    )\n\n    df = pl.scan_parquet(\n        uri,\n        hive_partitioning=len(remaining_partitions) &gt; 0,\n        hive_schema={\n            partition.column: partition.col_type\n            for partition in remaining_partitions\n        },\n        allow_missing_columns=True,\n        storage_options=storage_options,\n    )\n\n    if applied_filters:\n        # Add columns removed from partitions and added to uri\n        df = df.with_columns(\n            pl.lit(f.value)\n            .cast(\n                next(\n                    partition.col_type\n                    for partition in self.partitioning\n                    if partition.column == f.column\n                )\n            )\n            .alias(f.column)\n            for f in applied_filters\n        )\n\n    if remaining_filters:\n        filter_expr = _filters_to_expr(remaining_filters)\n        if filter_expr is not None:\n            df = df.filter(filter_expr)\n\n    if columns:\n        df = df.select(columns)\n\n    return df\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ParquetTable.__init__","title":"<code>__init__(name, uri, partitioning, partitioning_scheme=PartitioningScheme.DIRECTORY, description='', docs_filters=[], docs_columns=None, roapi_opts=None, parquet_file_name='df.parquet', table_metadata_args=None)</code>","text":"<p>Initialize the ParquetTable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the table, used for documentation and metadata.</p> required <code>uri</code> <code>str</code> <p>uri of the table, typically an S3 bucket path.</p> required <code>partitioning</code> <code>list[Partition]</code> <p>partitioning scheme for the table. This is a list of Partition objects, which define the columns and types used for partitioning</p> required <code>partitioning_scheme</code> <code>PartitioningScheme</code> <p>scheme used for partitioning. Defaults to PartitioningScheme.DIRECTORY.</p> <code>DIRECTORY</code> <code>description</code> <code>str</code> <p>description of the table, used for documentation. Defaults to \"\".</p> <code>''</code> <code>docs_filters</code> <code>list[Filter]</code> <p>documentation filters for the table. These filters are used to generate documentation and are not applied to the data. Defaults to [].</p> <code>[]</code> <code>docs_columns</code> <code>list[str] | None</code> <p>docsumentation columns for the table. These columns are used to generate documentation and are not applied to the data. Defaults to None.</p> <code>None</code> <code>roapi_opts</code> <code>RoapiOptions | None</code> <p>Read-only API options for the table. These options are used to configure the ROAPI endpoint for the table. Defaults to None.</p> <code>None</code> <code>parquet_file_name</code> <code>str</code> <p>parquet file name to use when building file fragments.</p> <code>'df.parquet'</code> <code>table_metadata_args</code> <code>dict[str, Any] | None</code> <p>additional metadata arguments for the table.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the partitioning_scheme is not a valid PartitioningScheme.</p> Source code in <code>src/datarepo/core/tables/parquet_table.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    uri: str,\n    partitioning: list[Partition],\n    partitioning_scheme: PartitioningScheme = PartitioningScheme.DIRECTORY,\n    description: str = \"\",\n    docs_filters: list[Filter] = [],\n    docs_columns: list[str] | None = None,\n    roapi_opts: RoapiOptions | None = None,\n    parquet_file_name: str = \"df.parquet\",\n    table_metadata_args: dict[str, Any] | None = None,\n):\n    \"\"\"Initialize the ParquetTable.\n\n    Args:\n        name (str): name of the table, used for documentation and metadata.\n        uri (str): uri of the table, typically an S3 bucket path.\n        partitioning (list[Partition]): partitioning scheme for the table.\n            This is a list of Partition objects, which define the columns and types used for partitioning\n        partitioning_scheme (PartitioningScheme, optional): scheme used for partitioning.\n            Defaults to PartitioningScheme.DIRECTORY.\n        description (str, optional): description of the table, used for documentation.\n            Defaults to \"\".\n        docs_filters (list[Filter], optional): documentation filters for the table.\n            These filters are used to generate documentation and are not applied to the data.\n            Defaults to [].\n        docs_columns (list[str] | None, optional): docsumentation columns for the table.\n            These columns are used to generate documentation and are not applied to the data.\n            Defaults to None.\n        roapi_opts (RoapiOptions | None, optional): Read-only API options for the table.\n            These options are used to configure the ROAPI endpoint for the table.\n            Defaults to None.\n        parquet_file_name (str, optional): parquet file name to use when building file fragments.\n        table_metadata_args (dict[str, Any] | None, optional): additional metadata arguments for the table.\n\n    Raises:\n        ValueError: if the partitioning_scheme is not a valid PartitioningScheme.\n    \"\"\"\n    if not isinstance(partitioning_scheme, PartitioningScheme):\n        raise ValueError(f\"Invalid partitioning scheme, got {partitioning_scheme}\")\n\n    self.name = name\n    self.uri = uri\n    self.partitioning = partitioning\n    self.partitioning_scheme = partitioning_scheme\n\n    self.table_metadata = TableMetadata(\n        table_type=\"PARQUET\",\n        description=description,\n        docs_args={\"filters\": docs_filters, \"columns\": docs_columns},\n        roapi_opts=roapi_opts,\n        **(table_metadata_args or {}),\n    )\n\n    self.parquet_file_name = parquet_file_name\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ParquetTable.build_file_fragment","title":"<code>build_file_fragment(filters)</code>","text":"<p>Returns a file path from the base table URI with the given filters. This will raise an error if the filter does not specify all partitions.</p> <p>This is currently used to generate the file path used by ROAPI to infer schemas.</p> Source code in <code>src/datarepo/core/tables/parquet_table.py</code> <pre><code>def build_file_fragment(self, filters: list[Filter]) -&gt; str:\n    \"\"\"\n    Returns a file path from the base table URI with the given filters.\n    This will raise an error if the filter does not specify all partitions.\n\n    This is currently used to generate the file path used by ROAPI to infer schemas.\n    \"\"\"\n    uri_with_prefix, partitions, _, _ = self._build_uri_from_filters(\n        normalize_filters(filters), include_base_uri=False\n    )\n    if len(partitions) &gt; 0:\n        partition_names = [partition.column for partition in partitions]\n        raise ValueError(\n            f\"Not enough partitions specified, missing: {partition_names}\"\n        )\n\n    return path.join(uri_with_prefix, self.parquet_file_name)\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.ParquetTable.get_schema","title":"<code>get_schema()</code>","text":"<p>Generates the schema of the table, including partitions and columns.</p> <p>Returns:</p> Name Type Description <code>TableSchema</code> <code>TableSchema</code> <p>table schema containing partitions and columns.</p> Source code in <code>src/datarepo/core/tables/parquet_table.py</code> <pre><code>def get_schema(self) -&gt; TableSchema:\n    \"\"\"Generates the schema of the table, including partitions and columns.\n\n    Returns:\n        TableSchema: table schema containing partitions and columns.\n    \"\"\"\n    partitions = [\n        TablePartition(\n            column_name=filter.column,\n            type_annotation=type(filter.value).__name__,\n            value=filter.value,\n        )\n        for filter in self.table_metadata.docs_args.get(\"filters\", [])\n    ]\n\n    # Pop the selected columns so that we still pull the full schema below\n    docs_args = {**self.table_metadata.docs_args}\n    docs_args.pop(\"columns\")\n\n    columns = []\n    if docs_args or not partitions:\n        table: NlkDataFrame = self(**docs_args)\n        columns = [\n            TableColumn(\n                column=key,\n                type=type.__str__(),\n                readonly=False,\n                filter_only=False,\n                has_stats=False,\n            )\n            for key, type in table.schema.items()\n        ]\n\n    return TableSchema(partitions=partitions, columns=columns)\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.PartitioningScheme","title":"<code>PartitioningScheme</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Defines the partitioning scheme for the table.</p> <p>DIRECTORY - e.g. s3://bucket/5956/2024-03-24 HIVE - e.g. s3://bucket/implant_id=5956/date=2024-03-24</p> Source code in <code>src/datarepo/core/tables/util.py</code> <pre><code>class PartitioningScheme(Enum):\n    \"\"\"\n    Defines the partitioning scheme for the table.\n\n    DIRECTORY - e.g. s3://bucket/5956/2024-03-24\n    HIVE - e.g. s3://bucket/implant_id=5956/date=2024-03-24\n    \"\"\"\n\n    DIRECTORY = 1\n    HIVE = 2\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.TableMetadata","title":"<code>TableMetadata</code>  <code>dataclass</code>","text":"<p>Information about a table used for documentation generation.</p> Source code in <code>src/datarepo/core/tables/metadata.py</code> <pre><code>@dataclass\nclass TableMetadata:\n    \"\"\"\n    Information about a table used for documentation generation.\n    \"\"\"\n\n    table_type: str\n    description: str\n    docs_args: Dict[str, Any]\n    latency_info: str | None = None\n    example_notebook: str | None = None\n    data_input: str | None = None\n    is_deprecated: bool = False\n    roapi_opts: RoapiOptions | None = None\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.TableProtocol","title":"<code>TableProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/datarepo/core/tables/metadata.py</code> <pre><code>class TableProtocol(Protocol):\n    # Properties used to generate the web catalog &amp; roapi config\n    table_metadata: TableMetadata\n\n    def __call__(self, **kwargs: Dict[str, Any]) -&gt; NlkDataFrame: ...\n\n    def get_schema(self) -&gt; TableSchema:\n        \"\"\"\n        Returns the schema of the table, used to generate the web catalog.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.TableProtocol.get_schema","title":"<code>get_schema()</code>","text":"<p>Returns the schema of the table, used to generate the web catalog.</p> Source code in <code>src/datarepo/core/tables/metadata.py</code> <pre><code>def get_schema(self) -&gt; TableSchema:\n    \"\"\"\n    Returns the schema of the table, used to generate the web catalog.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.TableSchema","title":"<code>TableSchema</code>  <code>dataclass</code>","text":"<p>TableSchema represents the schema of a table, including partitions and columns.</p> Source code in <code>src/datarepo/core/tables/metadata.py</code> <pre><code>@dataclass\nclass TableSchema:\n    \"\"\"TableSchema represents the schema of a table, including partitions and columns.\"\"\"\n\n    partitions: list[TablePartition]\n    columns: list[TableColumn]\n</code></pre>"},{"location":"api-docs/#datarepo.core.tables.table","title":"<code>table(*args, **kwargs)</code>","text":"<p>Decorator to define a table using a function.</p> Example uage <pre><code>@table(description=\"This is a sample table.\")\ndef my_table_function(param1, param2):\n    # Function logic to create a table\n    return NlkDataFrame(...)\n</code></pre> <p>Returns:</p> Type Description <code>Callable[[U], U] | Callable[[Any], Callable[[U], U]]</code> <p>Callable[[U], U] | Callable[[Any], Callable[[U], U]]: A decorator that wraps a function to create a table.</p> Source code in <code>src/datarepo/core/tables/decorator.py</code> <pre><code>def table(*args, **kwargs) -&gt; Callable[[U], U] | Callable[[Any], Callable[[U], U]]:\n    \"\"\"Decorator to define a table using a function.\n\n    Example uage:\n        ``` py\n        @table(description=\"This is a sample table.\")\n        def my_table_function(param1, param2):\n            # Function logic to create a table\n            return NlkDataFrame(...)\n        ```\n\n    Returns:\n        Callable[[U], U] | Callable[[Any], Callable[[U], U]]: A decorator that wraps a function to create a table.\n    \"\"\"\n\n    def wrapper(func):\n        return FunctionTable(\n            table_metadata=TableMetadata(\n                table_type=\"FUNCTION\",\n                description=func.__doc__.strip() if func.__doc__ else \"\",\n                docs_args=kwargs.get(\"docs_args\", {}),\n                latency_info=kwargs.get(\"latency_info\"),\n                example_notebook=kwargs.get(\"example_notebook\"),\n                data_input=kwargs.get(\"data_input\"),\n                is_deprecated=kwargs.get(\"is_deprecated\", False),\n            ),\n            func=func,\n        )\n\n    if len(args) == 0:\n        return wrapper\n    else:\n        return wrapper(args[0])\n</code></pre>"},{"location":"catalog/","title":"Web Catalog Example","text":"<p>See the example catalog here. This catalog is based off of the TPC-H Decision Support Benchmark.</p>"},{"location":"user-guide/","title":"User Guide","text":""},{"location":"user-guide/#core-concepts","title":"Core concepts","text":""},{"location":"user-guide/#tables","title":"Tables","text":"<p>A table in datarepo is a Python function that returns an <code>NlkDataFrame</code>. An <code>NlkDataFrame</code> is a thin wrapper of the polars LazyFrame. Tables are the fundamental building blocks for accessing and querying data. Tables can be backed by DeltaLake tables, Parquet tables, or pure Python functions.</p>"},{"location":"user-guide/#delta-lake-tables","title":"Delta Lake tables","text":"<pre><code>from datarepo.core import DeltalakeTable\nimport pyarrow as pa\n\n# Define the schema\nschema = pa.schema([\n    (\"p_partkey\", pa.int64()),\n    (\"p_name\", pa.string()),\n    (\"p_mfgr\", pa.string()),\n    (\"p_brand\", pa.string()),\n    (\"p_type\", pa.string()),\n    (\"p_size\", pa.int32()),\n    (\"p_container\", pa.string()),\n    (\"p_retailprice\", pa.decimal128(12, 2)),\n    (\"p_comment\", pa.string()),\n])\n\n# Create the table\npart = DeltalakeTable(\n    name=\"part\",\n    uri=\"s3://my-bucket/tpc-h/part\",\n    schema=schema,\n    docs_filters=[\n        Filter(\"p_partkey\", \"=\", 1),\n        Filter(\"p_brand\", \"=\", \"Brand#1\"),\n    ],\n    unique_columns=[\"p_partkey\"],\n    description=\"\"\"\n    Part information from the TPC-H benchmark.\n    Contains details about parts including name, manufacturer, brand, and retail price.\n    \"\"\",\n    table_metadata_args={\n        \"data_input\": \"Part catalog data from manufacturing systems, updated daily\",\n        \"latency_info\": \"Daily batch updates from manufacturing ERP system\",\n        \"example_notebook\": \"https://example.com/notebooks/part_analysis.ipynb\",\n    },\n)\n</code></pre>"},{"location":"user-guide/#parquet-tables","title":"Parquet tables","text":"<pre><code>from datarepo.core import ParquetTable, Partition, PartitioningScheme\nimport pyarrow as pa\n\n# Create the table\npartsupp = ParquetTable(\n    name=\"partsupp\",\n    uri=\"s3://my-bucket/tpc-h/partsupp\",\n    partitioning=[\n        Partition(column=\"ps_partkey\", col_type=pl.Int64),\n        Partition(column=\"ps_suppkey\", col_type=pl.Int64),\n    ],\n    partitioning_scheme=PartitioningScheme.HIVE,\n    docs_filters=[\n        Filter(\"ps_partkey\", \"=\", 1),\n        Filter(\"ps_suppkey\", \"=\", 1),\n    ],\n    description=\"\"\"\n    Part supplier information from the TPC-H benchmark.\n    Contains details about parts supplied by suppliers including available quantity and supply cost.\n    \"\"\",\n    table_metadata_args={\n        \"data_input\": \"Supplier inventory and pricing data from procurement systems\",\n        \"latency_info\": \"Real-time updates from supplier inventory management systems\",\n        \"example_notebook\": \"https://example.com/notebooks/supplier_analysis.ipynb\",\n    },\n)\n</code></pre>"},{"location":"user-guide/#function-tables","title":"Function tables","text":"<p>Function tables are created using the <code>@table</code> decorator and allow you to define custom data access logic:</p> <pre><code>from datarepo.core import table\nimport polars as pl\n\n@table(\n    data_input=\"Supplier master data from vendor management system &lt;code&gt;/api/suppliers/master&lt;/code&gt; endpoint\",\n    latency_info=\"Updated weekly by the supplier_master_sync DAG on Airflow\",\n)\ndef supplier() -&gt; NlkDataFrame:\n    \"\"\"Supplier information from the TPC-H benchmark.\"\"\"\n    data = {\n        \"s_suppkey\": [1, 2, 3, 4, 5],\n        \"s_name\": [\"Supplier#1\", \"Supplier#2\", \"Supplier#3\", \"Supplier#4\", \"Supplier#5\"],\n        \"s_address\": [\"123 Main St\", \"456 Oak Ave\", \"789 Pine Rd\", \"321 Elm St\", \"654 Maple Dr\"],\n        \"s_nationkey\": [1, 1, 2, 2, 3],\n        \"s_phone\": [\"555-0001\", \"555-0002\", \"555-0003\", \"555-0004\", \"555-0005\"],\n        \"s_acctbal\": [1000.00, 2000.00, 3000.00, 4000.00, 5000.00],\n        \"s_comment\": [\"Comment 1\", \"Comment 2\", \"Comment 3\", \"Comment 4\", \"Comment 5\"]\n    }\n    return NlkDataFrame(data)\n</code></pre>"},{"location":"user-guide/#databases","title":"Databases","text":"<p>A datarepo database is a Python module that contains tables. There are two main ways to create databases:</p>"},{"location":"user-guide/#module-database","title":"Module database","text":"<p>A module database wraps a Python module containing table definitions:</p> <pre><code># tpch_tables.py\nfrom datarepo.core import table\n\n@table\ndef supplier():\n    \"\"\"Supplier information.\"\"\"\n    return NlkDataFrame(...)\n\n@table\ndef partsupp():\n    \"\"\"Part supplier relationship information.\"\"\"\n    return NlkDataFrame(...)\n\n# Using the database\nfrom datarepo.core import ModuleDatabase\nimport tpch_tables\n\ndb = ModuleDatabase(tpch_tables)\n\n# Query data\n&gt;&gt;&gt; df = db.supplier()\n&gt;&gt;&gt; df.head()\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 s_suppkey\u2502 s_name    \u2502 s_address  \u2502 s_nationkey\u2502 s_phone  \u2502 s_acctbal\u2502 s_comment\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 Supplier#1\u2502 123 Main St\u2502 1          \u2502 555-0001 \u2502 1000.00  \u2502 Comment 1\u2502\n\u2502 2        \u2502 Supplier#2\u2502 456 Oak Ave\u2502 1          \u2502 555-0002 \u2502 2000.00  \u2502 Comment 2\u2502\n\u2502 3        \u2502 Supplier#3\u2502 789 Pine Rd\u2502 2          \u2502 555-0003 \u2502 3000.00  \u2502 Comment 3\u2502\n\u2502 4        \u2502 Supplier#4\u2502 321 Elm St \u2502 2          \u2502 555-0004 \u2502 4000.00  \u2502 Comment 4\u2502\n\u2502 5        \u2502 Supplier#5\u2502 654 Maple Dr\u2502 3         \u2502 555-0005 \u2502 5000.00  \u2502 Comment 5\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/#catalogs","title":"Catalogs","text":"<p>A catalog is a Python module that is a collection of databases.</p> <pre><code>from datarepo.core import Catalog, ModuleDatabase\nimport tpch_tables\n\n# Create a catalog\ndbs = {\"tpc-h\": ModuleDatabase(tpch_tables)}\nTPCHCatalog = Catalog(dbs)\n\n# Query data across databases\n&gt;&gt;&gt; supplier = TPCHCatalog.db(\"tpc-h\").supplier()\n&gt;&gt;&gt; partsupp = TPCHCatalog.db(\"tpc-h\").partsupp()\n\n# Join data across databases\n&gt;&gt;&gt; joined = supplier.join(partsupp, left_on=\"s_suppkey\", right_on=\"ps_suppkey\")\n&gt;&gt;&gt; joined.head()\nshape: (5, 12)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2510\n\u2502 s_suppkey\u2502 s_name    \u2502 s_address  \u2502 s_nationkey\u2502 s_phone  \u2502 s_acctbal\u2502 s_comment\u2502ps_partkey\u2502ps_suppkey\u2502ps_availqty \u2502ps_supplycost\u2502ps_comment \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1        \u2502 Supplier#1\u2502 123 Main St\u2502 1          \u2502 555-0001 \u2502 1000.00  \u2502 Comment 1\u2502 1        \u2502 1        \u2502 100        \u2502 100.00      \u2502 Part 1    \u2502\n\u2502 2        \u2502 Supplier#2\u2502 456 Oak Ave\u2502 1          \u2502 555-0002 \u2502 2000.00  \u2502 Comment 2\u2502 2        \u2502 2        \u2502 200        \u2502 200.00      \u2502 Part 2    \u2502\n\u2502 3        \u2502 Supplier#3\u2502 789 Pine Rd\u2502 2          \u2502 555-0003 \u2502 3000.00  \u2502 Comment 3\u2502 3        \u2502 3        \u2502 300        \u2502 300.00      \u2502 Part 3    \u2502\n\u2502 4        \u2502 Supplier#4\u2502 321 Elm St \u2502 2          \u2502 555-0004 \u2502 4000.00  \u2502 Comment 4\u2502 4        \u2502 4        \u2502 400        \u2502 400.00      \u2502 Part 4    \u2502\n\u2502 5        \u2502 Supplier#5\u2502 654 Maple D\u2502  3         \u2502 555-0005 \u2502 5000.00  \u2502 Comment 5\u2502 5        \u2502 5        \u2502 500        \u2502 500.00      \u2502 Part 5    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/#querying-data","title":"Querying data","text":"<p>datarepo provides a consistent interface for querying data across all table types:</p> <pre><code># Filter data\n&gt;&gt;&gt; df = db.supplier(filters=[(\"s_nationkey\", \"=\", 1)])\n\n# Select columns\n&gt;&gt;&gt; df = db.supplier(columns=[\"s_suppkey\", \"s_name\"])\n\n# Complex queries\n&gt;&gt;&gt; df = db.supplier(\n...     filters=[\n...         (\"s_nationkey\", \"=\", 1),\n...         (\"s_acctbal\", \"&gt;=\", 1000.00),\n...     ],\n...     columns=[\"s_suppkey\", \"s_name\", \"s_acctbal\"],\n... )\n</code></pre>"},{"location":"user-guide/#advanced-features","title":"Advanced features","text":""},{"location":"user-guide/#caching","title":"Caching","text":"<p>DeltaLake tables support caching to improve performance:</p> <pre><code>from datarepo.core.tables import DeltaCacheOptions\n\n# Configure caching\ncache_options = DeltaCacheOptions(\n    file_cache_path=\"~/.datarepo/cache\",\n    file_cache_last_checkpoint_valid_duration=\"30m\",\n)\n\n# Use caching\n&gt;&gt;&gt; df = db.supplier(cache_options=cache_options)\n</code></pre>"},{"location":"user-guide/#custom-columns","title":"Custom columns","text":"<p>You can add custom computed columns to tables:</p> <pre><code># Add a custom column\nsupplier = DeltalakeTable(\n    name=\"supplier\",\n    uri=\"s3://my-bucket/tpc-h/supplier\",\n    schema=schema,\n    extra_cols=[\n        (pl.col(\"s_acctbal\") * 1.1, \"s_acctbal_with_tax\"),\n    ],\n)\n\n# Query with custom column\n&gt;&gt;&gt; df = supplier(columns=[\"s_suppkey\", \"s_acctbal_with_tax\"])\n</code></pre>"}]}